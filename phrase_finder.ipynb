{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple years back, I stumbled upon Greenville Kleiser's *Fifteen Thousand Useful Phrases*, a book which a list of phrases that Kleiser had collected over the course of his lifetime. Though since it was published in 1917 some of the phrases are a bit antiquated, many of them stood out to me as quite eloquent or impressive with uses ranging from public speaking to conversation to literature writing. \n",
    "\n",
    "Though the utility of this book was initially quite high, I found it quite difficult at times to search through the entire list to find the phrase that perfectly described what I wanted to say. Thus in this project I sought out to search the database of phrases based on a metric of semantic similarity. That is, given an input word or phrase, I wanted to return a list of similar phrases from this database.\n",
    "\n",
    "This book is available via Project Gutenberg at www.gutenberg.org/files/18362/18362.txt\n",
    "\n",
    "We first begin by parsing the book and extracting phrases, using BeautifulSoup and regular expressions.\n",
    "\n",
    "The phrases we would like are of the form \n",
    "    `<p id=\"idxxxx\">phrase\\</p>`\n",
    "where the xxxx corresponds to the number of the phrase, starting at 0082 and ending at 15977."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "phrase_filename = 'phrases.html'\n",
    "file = open(phrase_filename, 'r')\n",
    "soup = BeautifulSoup(file, 'html.parser')\n",
    "soup.find('div', attrs={})\n",
    "phrase_list = soup.find_all(lambda tag : tag.name == 'p' \n",
    "                           and int(tag['id'][2:])>=82 and int(tag['id'][2:]) <= 15977 and not tag.has_attr('style'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to extract each phrase from the list of tags and remove the clarifications in hard brackets to obtain our final list of phrases. We then save this to a file for future access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip(phrase):\n",
    "    '''\n",
    "    Removes the whitespace and clarifying brackets \n",
    "    from the end of an input phrase\n",
    "    '''\n",
    "    out = str(phrase.string)\n",
    "    hard_bracket_re = re.compile('\\\\[.*\\\\]')\n",
    "    out = hard_bracket_re.sub('', out)\n",
    "    out = out.strip()\n",
    "    return out\n",
    "stripped_list = list(map(strip,phrase_list))\n",
    "import pickle\n",
    "with open('parsed_phrases.txt', 'wb') as file_to_write:\n",
    "    pickle.dump(stripped_list, file_to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to analyze and classify our phrases by meaning. To do so, we use a trained dataset created by Google, which maps many of the words found in Google News archives to a 300-dimensional real vector space based on meaning. In order to interface with this dataset, we use the library gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "googledata_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "googledata_model = models.KeyedVectors.load_word2vec_format(googledata_path,\n",
    "                                                                binary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the vector associated with a word we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0005188   0.16015625  0.0016098   0.02539062  0.09912109 -0.0859375\n",
      "  0.32421875 -0.02172852  0.13476562  0.11035156]\n"
     ]
    }
   ],
   "source": [
    "from gensim import parsing\n",
    "import numpy\n",
    "word1 = 'happy'\n",
    "word_vector1 = googledata_model[word1]\n",
    "word2 = 'sad'\n",
    "word_vector2 = googledata_model[word2]\n",
    "word3 = 'oscilloscope'\n",
    "word_vector3 = googledata_model[word3]\n",
    "print(word_vector1[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we show the first ten of three hundred values corresponding to the vector for happy.\n",
    "\n",
    "A reasonable metric of similarity between two word vectors is the dot product. Two vectors which point nearly in the same direction will have a large dot product, while vectors which are orthogonal will have a dot product of zero, indicating the absence of a semantic relation. Since our vectors are not necessarily of unit length, we must normalize our output.\n",
    "\n",
    "In order extend our similarity metric from words to phrases, given a phrase, we let its vector be the arithmetic mean of its constituent word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity(vector1, vector2):\n",
    "    '''\n",
    "    Returns a similarity score for two vectors, ranging from -1 to 1,\n",
    "    where a score near 1 indicates high similarity and a score near or\n",
    "    less than zero indicates low similarity. Note that a score near -1 does\n",
    "    not indicate opposite meaning.\n",
    "    '''\n",
    "    return numpy.inner(vector1, vector2) / (numpy.linalg.norm(vector1) * numpy.linalg.norm(vector2))\n",
    "def phrase_vector(phrase):\n",
    "    '''\n",
    "    Turns a phrase into a vector formed by taking the arithmetic mean\n",
    "    of its constituent phrase vectors.\n",
    "    '''\n",
    "    stopwords = parsing.preprocessing.STOPWORDS\n",
    "    phrase_words = [word for word in phrase.lower().split() if word not in stopwords or True]\n",
    "    phrase_vector = numpy.zeros((300,)) #Each word in the googlenews model is represented as a 300 dimensional vector\n",
    "    for word in phrase_words:\n",
    "        try:\n",
    "            word_vector = googledata_model[word]\n",
    "        except:\n",
    "            word_vector = numpy.zeros(300,)\n",
    "        phrase_vector = numpy.add(word_vector, phrase_vector)\n",
    "    phrase_vector = phrase_vector * 1/len(phrase_words)\n",
    "    return phrase_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our similarity metric from above, we see that the words 'happy' and 'sad' are somewhat similar, as they both are emotions. The word 'chair' however is very unrelated to both 'happy' and 'sad' and thus has a very low similarity score when compared to either word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.535461\n",
      "-0.0108541\n"
     ]
    }
   ],
   "source": [
    "print(similarity(word_vector1,word_vector2))\n",
    "print(similarity(word_vector1,word_vector3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read the list of fifteen thousand phrases from before and convert the list of phrases into a list of phrase vectors as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_to_read = 'parsed_phrases.txt'\n",
    "with open('parsed_phrases.txt', 'rb') as file_to_read:\n",
    "    interesting_phrases = pickle.load(file_to_read)\n",
    "interesting_phrases_vectors = list(map(phrase_vector, interesting_phrases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a function to get the phrases from our list of fifteen thousand phrases, which are most semantically similar to our input phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comparisons(phrase, number):\n",
    "    numbered_vectors = enumerate(interesting_phrases_vectors)\n",
    "    #Now we remove the ones where null\n",
    "    numbered_vectors = [item for item in numbered_vectors if item[-1] is not None]\n",
    "    vector_to_compare = phrase_vector(phrase)\n",
    "    for index in range(len(numbered_vectors)):\n",
    "        vector = numbered_vectors[index]\n",
    "        numbered_vectors[index] = tuple([vector[0],similarity(vector[-1], vector_to_compare)])\n",
    "    sorted_vectors = sorted(numbered_vectors, key = lambda item : -item[-1])\n",
    "    output = []\n",
    "    for index in range(number):\n",
    "        output.append(interesting_phrases[sorted_vectors[index][0]])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now using the above function, we search through our list of phrases as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['old and decrepit', 'childhood, youth, manhood, and age', 'man of iron', 'A man of imperious will', 'juvenile and budding', 'impoverished age', 'A fitful boy full of dreams and hopes', 'A grave man of pretending exterior', 'All embrowned and mossed with age', 'precocious wisdom', 'enfeebled by age', 'guide, philosopher, and friend', 'comely and vivacious', 'problematic age', 'He was giving his youth away by handfuls', 'Beguiled the weary soul of man', 'schooled in self-restraint', 'termagant wife', 'He was born to a lively and intelligent patriotism', 'boyish appreciation', 'senile sensualist', 'sprightly talk', 'ardent and aspiring', 'Dreams that fade and die in the dim west', 'unlettered laborer', 'ancient and venerable', 'vanished centuries', 'Enduring with smiling composure\\nthe near presence of people who are distasteful', 'A nameless sadness which is always born of moonlight', 'gaunt and ghastly']\n",
      "['cautious and reticent', 'apprehensive and anxious', 'slow and sluggish', 'critical and skeptical', 'cautious skepticism', 'coy reluctance', 'optimistic and reassuring', 'lukewarm and indifferent', 'shy and subdued', 'apologetic and uneasy', 'disturbed and anxious', 'scrupulous and anxious', 'apprehensive dread', 'inexperienced and timid', 'reluctant tolerance', 'pessimistic and disenchanted', 'pessimistic skepticism', 'cool and indifferent', 'skeptical contempt', 'An optimistic after-dinner mood', 'embarrassed and concerned', 'Reluctant as we are to believe', 'quiet cynicism', 'baffled and disappointed', 'frustrated and confounded', 'slow stupefaction', 'Apprehensive solicitude about the future', 'serene and quiet', 'dormant and subdued', 'benign and hopeful']\n"
     ]
    }
   ],
   "source": [
    "print(get_comparisons(\"old\", 30))\n",
    "print(get_comparisons(\"skeptical quiet hesitant slow\", 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, some of the phrases returned are quite useful, often differing from the text of the phrase such that they could not be found with a simple search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, this demonstration uses quite a bit of memory and is not particularly suitable for inexpensive deployment, though it would be cool to have as a web app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
